# NLP Techniques - Stemming, Lemmatization, BOW, TFIDF, Word2Vec

## Overview
This Jupyter Notebook explores fundamental Natural Language Processing (NLP) techniques used for text preprocessing and feature extraction. It covers:

- **Tokenization**: Splitting text into smaller units (tokens)
- **Stemming**: Reducing words to their root form
- **Lemmatization**: Converting words to their base dictionary form
- **Bag of Words (BOW)**: Representing text data in numerical form
- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Measuring the importance of words in a document
- **Word2Vec**: A word embedding technique that captures semantic relationships

## Requirements
To run this notebook, install the following dependencies:

```bash
pip install nltk sklearn gensim
```

## Usage
1. Open the notebook in Jupyter:
   ```bash
   jupyter notebook Stemming_lemmentization_BOW_TFIDF_Word2Vec.ipynb
   ```
2. Run the cells step by step to understand each NLP technique.
3. Modify the input text samples to experiment with different datasets.

## Implementation Details
- **NLTK** is used for tokenization, stemming, and lemmatization.
- **Scikit-learn** is used for BOW and TF-IDF vectorization.
- **Gensim** is used for Word2Vec implementation.

